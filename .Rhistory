# remove Trophic interaction
Cand.mod[[4]] <- update(Cand.mod[[1]], ~. - year0Z:Trophic)
# remove initialRichLN interaction
Cand.mod[[5]] <- update(Cand.mod[[1]], ~. - year0Z:initialDiv)
# remove duration interaction
Cand.mod[[6]] <- update(Cand.mod[[1]], ~. - year0Z:durationZ)
# year only
Cand.mod[[7]] <- lme(fixed = div ~ year0Z,
data = lmeDat, method = "ML",
random =  list(rand1, rand2, rand4),
correlation = corAR1(), weights = varExp())
# null model
Cand.mod[[8]] <- lme(fixed = div ~ 1,
data = lmeDat, method = "ML",
random =  list(rand1, rand2, rand4),
correlation = corAR1(), weights = varExp())
#########	#########
# use AICc when ratio of n/K is < 40
dim(lmeDat)[1]/22 # K = 20; can use AIC
#create a vector of names to trace back models in set
mod_numbers <- paste("Cand.mod", 1:length(Cand.mod), sep=" ")
mod_text <- c("Full model", "W/O Year:Scale", "W/O Year:Driver",
"W/O Year:Trophic Level", "W/O Year:Initial H'",
"W/O Year:Study Duration", "Year only", "Null model")
#generate AICc table with names
mod.aicctab <- aictab(cand.set= Cand.mod, modnames= mod_text, sort=TRUE, second.ord=FALSE) # second.ord =TRUE means AICc is used (not AIC)
print(mod.aicctab, digits=2, LL=TRUE)
source("./bts_Fig2.R")
qplot(fitSlope, data = slopeRich)
head(slopeRich)
source("./bts_Fig2.R")
qplot(fitSlope, data = slopeRich)
# select model to use for error bars
mod <- richGlobal
summary(mod)
intervals(mod)$fixed
intervals(mod, which = 'var-cov')
# get estimate of the sd of year (random)
yearSD <- intervals(mod, which = 'var-cov')$reStruct$subSiteID["sd(year0Z", "est."]
# CI is 1.96*SD
yearCI <- 1.96*yearSD
yearCI # approximate 95% CI for the random effect
df1 <- slopeRich[with(slopeRich, order(fitSlope)), ]
df1$newRow <- seq(1:dim(df1)[1])
df1$betaUpper <- df1$fitSlope + yearCI
df1$betaLower <- df1$fitSlope - yearCI
sigUpper <- dim(df1[df1$betaLower > 0, ])[1]
sigLower <- dim(df1[df1$betaUpper < 0, ])[1]
sigUpper; sigLower
sigUpper/dim(df1)[1] # 16% sig pos
sigLower/dim(df1)[1] # 3% sig neg
sigPercentage <- (sigUpper + sigLower)/dim(df1)[1]; sigPercentage
# caterpillar plot
no_grid <- theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank())
yAxis1 <- theme(axis.title.y = element_text(size = 16), axis.text.y = element_text(size = 14))
xAxis1 <- theme(axis.title.x = element_text(size = 16), axis.text.x = element_text(size = 14))
plotSpecs <- theme_bw() + yAxis1 + xAxis1 + no_grid
ggplot(df1, aes(newRow, fitSlope)) +
geom_errorbar(aes(ymin = betaLower, ymax = betaUpper),
size = 0.03, width = 0.0,
color = "black") + geom_point(size = 1) +
geom_hline(yintercept = 0, color = "darkgray", linetype = "dashed") +
plotSpecs +
xlab("Time-series") + ylab("Slope coefficient (year)")
qplot(fitSlope, data = slopeDiv)
# select model to use for error bars
mod <- divGlobal
summary(mod)
intervals(mod)$fixed
intervals(mod, which = 'var-cov')
# get estimate of the sd of year (random)
yearSD <- intervals(mod, which = 'var-cov')$reStruct$subSiteID["sd(year0Z", "est."]
# CI is 1.96*SD
yearCI <- 1.96*yearSD
yearCI # approximate 95% CI for the random effect
df1 <- slopeDiv[with(slopeDiv, order(fitSlope)), ]
df1$newRow <- seq(1:dim(df1)[1])
df1$betaUpper <- df1$fitSlope + yearCI
df1$betaLower <- df1$fitSlope - yearCI
sigUpper <- dim(df1[df1$betaLower > 0, ])[1]
sigLower <- dim(df1[df1$betaUpper < 0, ])[1]
sigUpper; sigLower
sigUpper/dim(df1)[1] # 6% sig pos
sigLower/dim(df1)[1] # 3% sig neg
sigPercentage <- (sigUpper + sigLower)/dim(df1)[1]; sigPercentage
# caterpillar plot
ggplot(df1, aes(newRow, fitSlope)) +
geom_errorbar(aes(ymin = betaLower, ymax = betaUpper),
size = 0.03, width = 0.0,
color = "black") + geom_point(size = 1) +
geom_hline(yintercept = 0, color = "darkgray", linetype = "dashed") +
plotSpecs +
xlab("Time-series") + ylab("Slope coefficient (year)")
source("./R/bts_dataPrep.R")
# detach dplyr
detach("package:dplyr", unload = TRUE)
library(nlme)
library(reshape2)
library(plyr)
# get stuff for lme models
source("./R/bts_lme_models.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(richDat$year0Z)
tail(richDat$year0Z)
# richness
dat <- richDat
dat$Prediction <- relevel(dat$Prediction, ref = "none")
############################################################
############################################################
#Get p-value
getTrtP <- function(newDF){
model.i <- lme(fixed = richLN ~ 1 +
year0Z*Scale + year0Z*Prediction +
year0Z*Trophic + year0Z*initialRichLN +
year0Z*durationZ,
data = newDF, method = "ML",
random =  list(rand1, rand2, rand4),
correlation = corAR1())
overallP <- anova(model.i)[9,4] # overall p value
levelP <- unname(summary(model.i)$tTable[12:14, 5]) # p values for each level
pVector <- c(overallP, levelP)
return(pVector)
}
# create vector of new predictions corresponding to length of old prediction
switchFactor3 <- function(x) {
predictionOld <- unique(x)
if(predictionOld == "negative") {predictionNew <- "neutral"}
if(predictionOld == "positive") {predictionNew <- "neutral"}
if(predictionOld == "neutral")
{predictionNew <- sample(c("negative", "positive"), 1)}
predictionVector <- rep(predictionNew, length(x))
return(predictionVector)}
# vector of time series names that were classified as neutral, positive, or negative
tsNames <- unique(dat[dat$Prediction != "none", ]$subSiteID)
tsNames
tsN <- length(tsNames); tsN
changeOneF <- function(dat) {
randSample <- sample(tsNames, 1)
randChange <- dat[dat$subSiteID == randSample, ]; dim(randChange)
unique(randChange$Prediction)
switchFactor(randChange)
predictionNew <- rep(switchFactor(randChange), length(randChange$Prediction))
randChange2 <- randChange; randChange2$Prediction <- predictionNew
randKeep <- dat[dat$subSiteID != randSample, ]; dim(randKeep)
newDF <- rbind(randKeep,  randChange2)
dim(newDF)
getTrtP(newDF)
}
system.time(changeOneF(dat)) # ~ 10 seconds, most/all of which is the lme call
randSample <- sample(tsNames, 50)
randSample
tsNames <- unique(dat[dat$Prediction != "none", ]$subSiteID)
tsNames
tsN <- length(tsNames); tsN
switcharoo <- function(dat, newPredictions) {
randSample <- sample(tsNames, newPredictions)
# create subset to change
randChange <- dat[dat$subSiteID %in% randSample, ]
# create subset to keep
randKeep <- dat[!dat$subSiteID %in% randSample, ]
# get list of new predictions
predictionList <- tapply(X = randChange$Prediction,
INDEX = randChange$subSiteID, FUN = switchFactor3)
#need to change from list to a vector/data frame
predictionNew <- factor(matrix(unlist(predictionList)))
# create new changed subset of data
randChange2 <- randChange
randChange2$Prediction <- predictionNew
# create the new data frame
newDF <- rbind(randKeep,  randChange2)
# run the model and get the pValue
getTrtP(newDF)
}
switcharoo(dat, 2)
dat <- divDat
# only one neutral prediction - lme won't run - need to use old classification
dat$PredictionOld <- droplevels(as.factor(gsub("neutral", "none", dat$Prediction)))
# relevel prediction
dat$PredictionOld <- relevel(dat$PredictionOld, ref = "none")
unique(dat$PredictionOld)
setwd("~/Desktop/currentResearch/1_biodiversityChange/bts_currentBiologyMS/biodiversity time series/bts data_master/bts_analysis CurrBio3")
rm(list=ls(all=TRUE)) # removes all previous material from R's memory
# get data for analysis
source("./R/bts_dataPrep_150620.R")
# detach dplyr
detach("package:dplyr", unload = TRUE)
library(nlme)
library(reshape2)
library(plyr)
# get stuff for lme models
source("./R/bts_lme_models_150618.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(divDat$year0Z)
tail(divDat$year0Z)
# diversity
dat <- divDat
unique(dat$PredictionOld)
unique(dat$PredictionOld)
setwd("~/Desktop/currentResearch/1_biodiversityChange/bts_currentBiologyMS/bts_gitHubRepo/analysis")
rm(list=ls(all=TRUE)) # removes all previous material from R's memory
# get data for analysis
source("./R/bts_dataPrep_150620.R")
# detach dplyr
detach("package:dplyr", unload = TRUE)
library(nlme)
library(reshape2)
library(plyr)
# Random effects for LME models
source("./R/bts_lme_models.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(divDat$year0Z)
tail(divDat$year0Z)
# diversity
dat <- divDat
setwd("~/Desktop/currentResearch/1_biodiversityChange/bts_currentBiologyMS/bts_gitHubRepo/analysis")
source("./R/bts_dataPrep.R")
# detach dplyr
detach("package:dplyr", unload = TRUE)
library(nlme)
library(reshape2)
library(plyr)
# Random effects for LME models
source("./R/bts_lme_models.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(divDat$year0Z)
tail(divDat$year0Z)
# diversity
dat <- divDat
unique(dat$Prediction)
with(dat, table(Prediction))
dat$PredictionOld <- droplevels(as.factor(gsub("none", "neutral", dat$Prediction)))
unique(dat$PredictionOld)
# relevel predictionOld
dat$PredictionOld <- relevel(dat$PredictionOld, ref = "neutral")
with(dat, table(Prediction))
with(dat, table(PredictionOld))
getTrtP <- function(newDF){
model.i <- lme(fixed = div ~ 1 +
year0Z*Scale + year0Z*PredictionOld +
year0Z*Trophic + year0Z*initialDiv +
year0Z*durationZ,
data = newDF, method = "ML",
random =  list(rand1, rand2, rand4),
correlation = corAR1(), weights = varExp())
overallP <- anova(model.i)[9,4] # overall p value
levelP <- unname(summary(model.i)$tTable[11:12, 5])
# p values for each level
pVector <- c(overallP, levelP)
return(pVector)
}
# create vector of new predictions corresponding to length of old prediction
switchFactor3 <- function(x) {
predictionOriginal <- unique(x)
if(predictionOriginal == "negative") {predictionNew <- "neutral"}
if(predictionOriginal == "positive") {predictionNew <- "neutral"}
if(predictionOriginal == "neutral")
{predictionNew <- sample(c("negative", "positive"), 1)}
predictionVector <- rep(predictionNew, length(x))
return(predictionVector)}
###############################
tsNames <- unique(dat[dat$Prediction != "none", ]$subSiteID)
tsNames <- unique(dat$subSiteID)
tsNames
tsN <- length(tsNames); tsN
switcharoo <- function(dat, newPredictions) {
randSample <- sample(tsNames, newPredictions)
# create subset to change
randChange <- dat[dat$subSiteID %in% randSample, ]
# create subset to keep
randKeep <- dat[!dat$subSiteID %in% randSample, ]
# get list of new predictions
predictionList <- tapply(X = randChange$PredictionOld,
INDEX = randChange$subSiteID, FUN = switchFactor3)
#need to change from list to a vector/data frame
predictionNew <- factor(matrix(unlist(predictionList)))
# create new changed subset of data
randChange2 <- randChange
randChange2$PredictionOld <- predictionNew
# create the new data frame
newDF <- rbind(randKeep,  randChange2)
# run the model and get the pValue
getTrtP(newDF)
}
predictionChanges <- c(1:13)
predictionChanges
AllReps <- predictionChanges
N <- length(AllReps); N
pValsList <- vector("list", length = N)
pValsList
names(pValsList) <- factor(predictionChanges)
str(pValsList)
pValsN <- 10 # number of times to replicate
switcharoo(dat, 4)
rm(list=ls(all=TRUE)) # removes all previous material from R's memory
# get data for analysis
source("./R/bts_dataPrep.R")
library(ggplot2)
library(nlme)
library(arm)
library(AICcmodavg)
# plotting functions
source("./R/Standard Graphical Pars_RE.R")
source("./R/multiplotF.R")
# Random effects for LME models
source("./R/bts_lme_models.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(richDat$year0Z)
tail(richDat$year0Z)
# Simplest model - what is the overall slope, accounting for random effects?
lmeDat <- richDat
simple <- lme(fixed = richLN ~ I(year0Z + 1990),
data = lmeDat, method = "REML",
random =  randList1,
correlation = corAR1())
summary(simple)
plot(simple)
summary(simple)
int <- fixed.effects(simple)[1]
slope <- fixed.effects(simple)[2]
ggDat <- lmeDat
ggDat$simple.fitted <- predict(simple)
ggDat$simple.resids <- with(ggDat, simple.fitted - richLN)
plot(simple.fitted ~ year0Z, ggDat)
plot(simple.resids ~ year0Z, ggDat)
year <- ggDat$year0 + 1962
#
no_legend <- theme(legend.position = "none")
theme1 <- theme_bw(base_size = 16) +
theme(axis.text = element_text(size = 14))
no_grid <- theme(panel.grid.major = element_blank()) +
theme(panel.grid.minor = element_blank())
# Points colored by studyName, but fitted lines by subSiteID
# With abundance, solid lines; w/o = dashed lines
richLNp2 <- ggplot(ggDat, aes(year0 + 1962, richLN)) + theme1 + no_legend +
geom_point(aes(color = studyName), size = 1, alpha = 0.8) +
geom_smooth(data = subset(ggDat, AbundYes == 1),
method = "lm", se = F, size = 0.15,
aes(group = subSiteID, color = studyName)) +
geom_smooth(data = subset(ggDat, AbundYes == 0),
method = "lm", se = F, size = 0.15, linetype = "dashed",
aes(group = subSiteID, color = studyName))
ULClabel <- theme(plot.title = element_text(hjust = -0.05, vjust = 1,
size = rel(1.5)))
Fig1B <- richLNp2 + xlab("Year") + ylab("ln (S)") +
labs(title = "B") + ULClabel +
geom_abline(intercept = int, slope = slope, color = "black", size = 0.3) + no_grid
exp(slope)
slope
setwd("~/Desktop/currentResearch/1_biodiversityChange/bts_currentBiologyMS/biodiversity time series/bts data_master/bts_analysis CurrBio3")
source("./R/bts_dataPrep_150620.R")
library(ggplot2)
library(nlme)
library(arm)
library(AICcmodavg)
# plotting functions
source("./R/Standard Graphical Pars_RE.R")
source("./R/multiplotF.R")
# get stuff for lme models
source("./R/bts_lme_models_150618.R")
# make sure the data are ordered by time within time-series (
# necessary for autocorrelation
head(richDat$RS.year0Z)
tail(richDat$RS.year0Z)
summary(richDat$year0)
summary(richDat$duration)
############################################################
############################################################
###FIGURE 1A: MAP
############################################################
#############################
# Simplest model - what is the overall slope, accounting for random effects?
lmeDat <- richDat
simple <- lme(fixed = richLN ~ I(year0Z + 1990),
data = lmeDat, method = "REML",
random =  randList1,
correlation = corAR1())
summary(simple)
simple <- lme(fixed = richLN ~ year0Z,
data = lmeDat, method = "REML",
random =  randList1,
correlation = corAR1())
summary(simple)
slope
exp(slope) - 0.1
exp()
ln(10)
log(10)
exp(2.302)
exp(slope) - 0.1
(exp(slope) - 0.1) * 10
setwd("~/github/marBiodivChange_impacts")
library(dplyr)
rm(list=ls(all=TRUE))
library(dplyr)
library(rgdal)
library(maptools)
library(ggplot2)
siteList <- read.csv("./data/elahi_extractedVals.csv")
unique(siteList$studyName)
# Need to remove large-scale studies that were not used in the CB analysis
sl2 <- siteList[siteList$studyName != "Keller" &
siteList$studyName != "Bebars" &
siteList$studyName != "Greenwood" &
siteList$studyName != "Sonnewald" &
siteList$studyName != "SwedFishTrawl"  , ]
unique(sl2$studyName)
sl2$impact <- with(sl2, ifelse(Impact.Score == -9999, "no", "yes"))
# Categorize impact score or not, rename and select columns
sl2$impact <- with(sl2, ifelse(Impact.Score == -9999, "no", "yes"))
sl2 <- sl2 %>% rename(Long = Long_, impactScore = Impact.Score,
originalID = OBJECTID..) %>%
select(originalID, studyName, Lat, Long, impactScore, impact)
head(sl2)
nudged <- readOGR(dsn = "./data/sites_nudged.kml",
layer = "sites_nudged")
summary(nudged)
nudged
glimpse(nudged)
nudgedDF <- as.data.frame(nudged)
nudgedDF
nudgedDF <- nudgedDF %>% rename(originalID = Description)
nudgedDF
?readOGR
qplot(coords.x1, coords.x2, data = nudgedDF)
imp_map <- raster("../bigFiles/model_class_wgs84_lzw.tif")
library(raster)
imp_map <- raster("../bigFiles/model_class_wgs84_lzw.tif")
plot(imp_map)
plotRGB(imp_map)
imp_map_b1 <- raster("../bigFiles/model_class_wgs84_lzw.tif", band = 1)
imp_map_b2 <- raster("../bigFiles/model_class_wgs84_lzw.tif", band = 2)
imp_map_b3 <- raster("../bigFiles/model_class_wgs84_lzw.tif", band = 3)
# We can subsequently combine these into a stack which allows us to plot using
# the RGB colour scheme.
imp_stack <- stack(imp_map_b1, imp_map_b2, imp_map_b3)
plotRGB(imp_stack)
imp_level <- c('very low', 'low', 'medium', 'very high', 'land', 'sf_land', 'sf_bay')
lat <- c(-74.903173,  -6.680232,  18.728598, 58.438787, 45.213004, 37.22158,  38.065723)
long <- c(-45.068090, -105.823552, -110.122704, -0.357954, -116.556702, -122.202644, -122.392813)
test_vals <- data.frame("imp_level" = imp_level, "lat" = lat, "long" = long)
# Convert test_vals into a spatial object
# The assignments specifies which columns have your c(lat, long)
coordinates(test_vals) <- c(3, 2)
projection(test_vals) <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
impl <- extract(imp_stack, test_vals, buffer = 2000, small = T)
impl
plotRGB(imp_stack)
plotRGB(imp_stack, colNA = 'black')
points(test_vals)
plotRGB(imp_stack, add = TRUE)
plot(c(-170, 10), c(-90, 90), type='n', axes=F, xlab='', ylab='')
plotRGB(rgb,add=T)
plotRGB(imp_stack)
plotRGB(imp_stack, colNA = 'black')
points(test_vals)
imp_stack
sl2
test_vals
coordinates(test_vals)
imp_level <- c('very low', 'low', 'medium', 'very high', 'land', 'sf_land', 'sf_bay')
lat <- c(-74.903173,  -6.680232,  18.728598, 58.438787, 45.213004, 37.22158,  38.065723)
long <- c(-45.068090, -105.823552, -110.122704, -0.357954, -116.556702, -122.202644, -122.392813)
test_vals <- data.frame("imp_level" = imp_level, "lat" = lat, "long" = long)
test_vals
head(sl2)
rm(list=ls(all=TRUE))
library(dplyr)
library(rgdal)
#library(maptools)
#library(ggplot2)
siteList <- read.csv("./data/elahi_extractedVals.csv")
unique(siteList$studyName)
# Need to remove large-scale studies that were not used in the CB analysis
sl2 <- siteList[siteList$studyName != "Keller" &
siteList$studyName != "Bebars" &
siteList$studyName != "Greenwood" &
siteList$studyName != "Sonnewald" &
siteList$studyName != "SwedFishTrawl"  , ]
unique(sl2$studyName)
# Categorize impact score or not, rename and select columns
sl2$impact <- with(sl2, ifelse(Impact.Score == -9999, "no", "yes"))
sl2 <- sl2 %>% rename(Long = Long_, impactScore = Impact.Score,
originalID = OBJECTID..) %>%
select(originalID, studyName, Lat, Long, impactScore, impact)
head(sl2)
head(sl2)
rm(list=ls(all=TRUE))
library(dplyr)
library(rgdal)
#library(maptools)
#library(ggplot2)
siteList <- read.csv("./data/elahi_extractedVals.csv")
unique(siteList$studyName)
# Need to remove large-scale studies that were not used in the CB analysis
sl2 <- siteList[siteList$studyName != "Keller" &
siteList$studyName != "Bebars" &
siteList$studyName != "Greenwood" &
siteList$studyName != "Sonnewald" &
siteList$studyName != "SwedFishTrawl"  , ]
unique(sl2$studyName)
# Categorize impact score or not, rename and select columns
sl2$impact <- with(sl2, ifelse(Impact.Score == -9999, "no", "yes"))
head(sl2)
sl3 <- sl2 %>% rename(Long = Long_, impactScore = Impact.Score,
originalID = OBJECTID..) %>%
select(originalID, studyName, Lat, Long, impactScore, impact)
